{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-OaAom1tecBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fd5dac-1a22-4012-a3b5-067d6eb26d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc"
      ],
      "metadata": {
        "id": "OKL57UfpZmtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected columns\n",
        "program_cols = [\n",
        "    'UNITID', 'INSTNM', 'CIPCODE', 'CIPDESC', 'CREDLEV', 'CREDDESC',\n",
        "    'EARN_MDN_4YR', 'EARN_MDN_1YR', 'EARN_COUNT_WNE_4YR', 'EARN_COUNT_WNE_1YR',\n",
        "    'DISTANCE'\n",
        "]\n",
        "\n",
        "institution_cols = [\n",
        "    'UNITID', 'MD_EARN_WNE_P10', 'STABBR', 'REGION', 'CONTROL',\n",
        "    'COSTT4_A', 'TUITIONFEE_IN', 'C150_4', 'ADM_RATE', 'PCTPELL'\n",
        "]\n",
        "\n",
        "# Load programs\n",
        "df_programs = pd.read_csv(\n",
        "    '/content/drive/MyDrive/5450_Files/Most-Recent-Cohorts-Field-of-Study.csv',\n",
        "    usecols=program_cols,\n",
        "    low_memory=False\n",
        ")\n",
        "print(f\"Programs loaded: {len(df_programs):,}\")\n",
        "\n",
        "# Load institutions\n",
        "df_institutions = pd.read_csv(\n",
        "    '/content/drive/MyDrive/5450_Files/Most-Recent-Cohorts-Institution.csv',\n",
        "    usecols=institution_cols,\n",
        "    low_memory=False\n",
        ").drop_duplicates(subset='UNITID')\n",
        "print(f\"Institutions loaded: {len(df_institutions):,}\")"
      ],
      "metadata": {
        "id": "HjfVsgpVZut2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70658e26-fedf-47e7-c6e1-db3094323d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programs loaded: 229,188\n",
            "Institutions loaded: 6,429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge programs + institutions\n",
        "merged_df = df_programs.merge(df_institutions, on='UNITID', how='left')\n",
        "print(f\"Merged dataset: {len(merged_df):,} rows, {merged_df.shape[1]} columns\")\n",
        "\n",
        "# Clean up memory\n",
        "del df_programs, df_institutions\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0d9GrkuqZ4uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCleaning privacy-suppressed values...\")\n",
        "\n",
        "# Replace PS/PrivacySuppressed with NaN\n",
        "merged_df = merged_df.replace(['PrivacySuppressed', 'PS', 'NULL', 'null'], np.nan)\n",
        "\n",
        "print(\"Privacy-suppressed values cleaned up to NaN\")\n"
      ],
      "metadata": {
        "id": "qO98D1znZ9JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "id": "f-SDpeplaQeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nConverting columns to numeric...\")\n",
        "\n",
        "# Get numeric columns (exclude text columns)\n",
        "numeric_cols = [col for col in merged_df.columns\n",
        "                if col not in ['INSTNM', 'CIPDESC', 'CREDDESC', 'STABBR']]\n",
        "\n",
        "# Convert\n",
        "for col in numeric_cols:\n",
        "    merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
        "\n",
        "print(f\"Converted {len(numeric_cols)} columns to numeric\")"
      ],
      "metadata": {
        "id": "dbKtuawAaERf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "id": "HCalPPk_adty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_nops = merged_df.copy()  # \"nops\" = no privacy suppressed\n",
        "\n",
        "print(f\"\\n Cleaned dataset ready: {merged_df_nops.shape}\")\n",
        "print(f\"  Rows: {len(merged_df_nops):,}\")\n",
        "print(f\"  Columns: {merged_df_nops.shape[1]}\")"
      ],
      "metadata": {
        "id": "rB22Q6jvaibj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nData types:\")\n",
        "print(merged_df_nops.dtypes)"
      ],
      "metadata": {
        "id": "N1i_C3kQanB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMissing data summary:\")\n",
        "missing = merged_df_nops.isnull().sum()\n",
        "missing_pct = (missing / len(merged_df_nops) * 100).round(1)\n",
        "\n",
        "for col in merged_df_nops.columns:\n",
        "    if missing[col] > 0:\n",
        "        print(f\"  {col:25s}: {missing[col]:,} ({missing_pct[col]}%)\")"
      ],
      "metadata": {
        "id": "kv_TCAMzariy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_definitions = {\n",
        "    'UNITID': 'Institution ID',\n",
        "    'INSTNM': 'Institution name',\n",
        "    'CIPCODE': 'Major code',\n",
        "    'CIPDESC': 'Major name',\n",
        "    'CREDLEV': 'Credential level (1=Cert, 3=Bach, 5=Master, etc)',\n",
        "    'CREDDESC': 'Credential description',\n",
        "    'EARN_MDN_4YR': 'Median earnings 4 years after grad',\n",
        "    'EARN_MDN_1YR': 'Median earnings 1 year after grad',\n",
        "    'EARN_COUNT_WNE_4YR': 'Sample size for 4yr earnings',\n",
        "    'EARN_COUNT_WNE_1YR': 'Sample size for 1yr earnings',\n",
        "    'DISTANCE': 'Online program flag',\n",
        "    'STABBR': 'State',\n",
        "    'CONTROL': 'Institution type (1=Public, 2=Private, 3=For-profit)',\n",
        "    'REGION': 'Geographic region',\n",
        "    'ADM_RATE': 'Admission rate',\n",
        "    'COSTT4_A': 'Annual cost',\n",
        "    'TUITIONFEE_IN': 'Tuition',\n",
        "    'PCTPELL': 'Percent receiving Pell grants (low-income proxy)',\n",
        "    'C150_4': 'Graduation rate',\n",
        "    'MD_EARN_WNE_P10': 'School-wide median earnings (10yr)'\n",
        "}\n",
        "\n",
        "print(\"\\nColumn definitions:\")\n",
        "for col, definition in column_definitions.items():\n",
        "    print(f\"  {col:25s}: {definition}\")"
      ],
      "metadata": {
        "id": "R9LHPuHqa0BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "has_4yr = merged_df_nops['EARN_MDN_4YR'].notna()\n",
        "has_1yr = merged_df_nops['EARN_MDN_1YR'].notna()\n",
        "has_both = has_4yr & has_1yr\n",
        "has_either = has_4yr | has_1yr\n",
        "\n",
        "print(f\"\\nTotal programs: {len(merged_df_nops):,}\")\n",
        "print(f\"4-year earnings: {has_4yr.sum():,} ({has_4yr.sum()/len(merged_df_nops)*100:.1f}%)\")\n",
        "print(f\"1-year earnings: {has_1yr.sum():,} ({has_1yr.sum()/len(merged_df_nops)*100:.1f}%)\")\n",
        "print(f\"BOTH: {has_both.sum():,} ({has_both.sum()/len(merged_df_nops)*100:.1f}%)\")\n",
        "print(f\"EITHER: {has_either.sum():,} ({has_either.sum()/len(merged_df_nops)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n Imputation potential:\")\n",
        "print(f\"  Without imputation: {has_4yr.sum():,} programs\")\n",
        "print(f\"  With imputation: {has_either.sum():,} programs\")\n",
        "print(f\"  Gain: +{has_either.sum() - has_4yr.sum():,}\")"
      ],
      "metadata": {
        "id": "56amGIeia1Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get programs with BOTH 1yr and 4yr\n",
        "df_both = merged_df_nops[has_both].copy()\n",
        "print(f\"\\nPrograms with BOTH earnings: {len(df_both):,}\")"
      ],
      "metadata": {
        "id": "wiqReu31a-Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_both['GROWTH_FACTOR'] = df_both['EARN_MDN_4YR'] / df_both['EARN_MDN_1YR']\n",
        "\n",
        "print(f\"\\nGrowth factor stats:\")\n",
        "print(f\"  Median: {df_both['GROWTH_FACTOR'].median():.3f}x\")\n",
        "print(f\"  Mean: {df_both['GROWTH_FACTOR'].mean():.3f}x\")\n",
        "print(f\"  Min: {df_both['GROWTH_FACTOR'].min():.3f}x\")\n",
        "print(f\"  Max: {df_both['GROWTH_FACTOR'].max():.3f}x\")"
      ],
      "metadata": {
        "id": "rHj0Wuq6bBV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRemoving outliers...\")\n",
        "print(f\"Before: {len(df_both):,}\")\n",
        "\n",
        "# Keep reasonable range (0.5x to 3.0x)\n",
        "reasonable = (df_both['GROWTH_FACTOR'] >= 0.5) & (df_both['GROWTH_FACTOR'] <= 3.0)\n",
        "df_both = df_both[reasonable].copy()\n",
        "\n",
        "print(f\"After: {len(df_both):,}\")\n",
        "print(f\"Removed: {(~reasonable).sum():,}\")\n",
        "\n",
        "print(f\"\\nCleaned median: {df_both['GROWTH_FACTOR'].median():.3f}x\")"
      ],
      "metadata": {
        "id": "NGcZ9OTzbGIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why chose median growth factor?"
      ],
      "metadata": {
        "id": "5vwitCwmbTrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate by institution type\n",
        "growth_by_control = df_both.groupby('CONTROL')['GROWTH_FACTOR'].agg(['median', 'mean', 'count'])\n",
        "overall_growth = df_both['GROWTH_FACTOR'].median()\n",
        "\n",
        "control_names = {1: 'Public', 2: 'Private Nonprofit', 3: 'For-Profit'}\n",
        "\n",
        "print(f\"\\n{'Type':<22} {'Median':<10} {'Mean':<10} {'Count':<10}\")\n",
        "print(\"-\" * 52)\n",
        "\n",
        "for control in sorted(growth_by_control.index):\n",
        "    if not np.isnan(control):\n",
        "        name = control_names.get(control, f'Type {int(control)}')\n",
        "        med = growth_by_control.loc[control, 'median']\n",
        "        mn = growth_by_control.loc[control, 'mean']\n",
        "        cnt = int(growth_by_control.loc[control, 'count'])\n",
        "        print(f\"{name:<22} {med:<10.3f} {mn:<10.3f} {cnt:<10,}\")\n",
        "\n",
        "print(\"-\" * 52)\n",
        "print(f\"{'Overall':<22} {overall_growth:<10.3f} {df_both['GROWTH_FACTOR'].mean():<10.3f} {len(df_both):<10,}\")\n",
        "\n",
        "# Save for imputation\n",
        "growth_factors = {\n",
        "    'overall': overall_growth,\n",
        "    'by_control': growth_by_control['median'].to_dict()\n",
        "}\n",
        "\n",
        "print(\"\\nGrowth factors saved\")"
      ],
      "metadata": {
        "id": "Uvdd1rdkbJOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Distribution\n",
        "axes[0].hist(df_both['GROWTH_FACTOR'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(overall_growth, color='red', linestyle='--', linewidth=2,\n",
        "                label=f'Median: {overall_growth:.3f}x')\n",
        "axes[0].set_xlabel('Growth Factor')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Growth Factor Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# By institution type\n",
        "growth_medians = growth_by_control['median'].sort_values(ascending=False)\n",
        "axes[1].bar([control_names.get(c, str(int(c))) for c in growth_medians.index],\n",
        "            growth_medians.values, color=['green', 'blue', 'orange'], alpha=0.7)\n",
        "axes[1].set_ylabel('Median Growth Factor')\n",
        "axes[1].set_title('Growth by Institution Type')\n",
        "axes[1].axhline(overall_growth, color='red', linestyle='--', alpha=0.5)\n",
        "for i, v in enumerate(growth_medians.values):\n",
        "    axes[1].text(i, v + 0.01, f'{v:.3f}x', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('growth_factors.png', dpi=300)\n",
        "print(\"Saved: growth_factors.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r0VmuNHJmudQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print orignial 1 year too to make sense"
      ],
      "metadata": {
        "id": "QT9sCkb1o1Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tracking columns\n",
        "merged_df_nops['EARN_4YR_SOURCE'] = 'missing'\n",
        "merged_df_nops['EARN_MDN_4YR_FINAL'] = merged_df_nops['EARN_MDN_4YR'].copy()\n",
        "\n",
        "# Mark original 4-year data\n",
        "has_4yr = merged_df_nops['EARN_MDN_4YR'].notna()\n",
        "merged_df_nops.loc[has_4yr, 'EARN_4YR_SOURCE'] = 'original'\n",
        "\n",
        "print(f\"\\nOriginal 4-year data: {has_4yr.sum():,}\")\n",
        "\n",
        "# Programs needing imputation (have 1yr but not 4yr)\n",
        "needs_impute = (~has_4yr) & merged_df_nops['EARN_MDN_1YR'].notna()\n",
        "print(f\"Programs to impute: {needs_impute.sum():,}\")"
      ],
      "metadata": {
        "id": "Rhv2KySIof_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_count = 0\n",
        "\n",
        "for idx in merged_df_nops[needs_impute].index:\n",
        "    control_type = merged_df_nops.loc[idx, 'CONTROL']\n",
        "    earn_1yr = merged_df_nops.loc[idx, 'EARN_MDN_1YR']\n",
        "\n",
        "    # Use institution-specific growth factor\n",
        "    if pd.notna(control_type) and control_type in growth_factors['by_control']:\n",
        "        growth = growth_factors['by_control'][control_type]\n",
        "    else:\n",
        "        growth = growth_factors['overall']\n",
        "\n",
        "    merged_df_nops.loc[idx, 'EARN_MDN_4YR_FINAL'] = earn_1yr * growth\n",
        "    merged_df_nops.loc[idx, 'EARN_4YR_SOURCE'] = 'imputed'\n",
        "    imputed_count += 1\n",
        "\n",
        "    if imputed_count % 5000 == 0:\n",
        "        print(f\"  {imputed_count:,} / {needs_impute.sum():,}\")\n",
        "\n",
        "print(f\"\\nImputation complete: {imputed_count:,} programs\")"
      ],
      "metadata": {
        "id": "yws77u_Yon8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final summary\n",
        "source_counts = merged_df_nops['EARN_4YR_SOURCE'].value_counts()\n",
        "\n",
        "print(f\"\\nFinal earnings coverage:\")\n",
        "for source, count in source_counts.items():\n",
        "    pct = count / len(merged_df_nops) * 100\n",
        "    print(f\"  {source.capitalize():10s}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "total_usable = (merged_df_nops['EARN_4YR_SOURCE'] != 'missing').sum()\n",
        "print(f\"\\nTotal usable: {total_usable:,} ({total_usable/len(merged_df_nops)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "XnsDgdoso-c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "has_4yr = merged_df_nops['EARN_MDN_4YR_FINAL'].notna()\n",
        "print(f\"\\nOriginal 4-year data: {has_4yr.sum():,}\")"
      ],
      "metadata": {
        "id": "JUo-PVSppLrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_nops.info()"
      ],
      "metadata": {
        "id": "5Ck8LAVapX36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EDA: CORRELATION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Select numeric features\n",
        "numeric_features = [\n",
        "    'EARN_MDN_4YR', 'EARN_MDN_1YR', 'COSTT4_A', 'TUITIONFEE_IN',\n",
        "    'ADM_RATE', 'C150_4', 'PCTPELL', 'MD_EARN_WNE_P10',\n",
        "    'CREDLEV', 'CONTROL', 'REGION'\n",
        "]\n",
        "\n",
        "corr_matrix = merged_df_nops[numeric_features].corr()\n",
        "\n",
        "print(\"\\nHigh correlations (|r| > 0.7):\")\n",
        "high_corr = []\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
        "            high_corr.append({\n",
        "                'Feature_1': corr_matrix.columns[i],\n",
        "                'Feature_2': corr_matrix.columns[j],\n",
        "                'Correlation': corr_matrix.iloc[i, j]\n",
        "            })\n",
        "            print(f\"  {corr_matrix.columns[i]:20s} <-> {corr_matrix.columns[j]:20s}: r = {corr_matrix.iloc[i, j]:.3f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
        "axes[0].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Correlations with 4yr earnings\n",
        "earnings_corr = corr_matrix['EARN_MDN_4YR'].drop('EARN_MDN_4YR').sort_values()\n",
        "colors = ['red' if x < 0 else 'green' for x in earnings_corr.values]\n",
        "\n",
        "axes[1].barh(range(len(earnings_corr)), earnings_corr.values, color=colors, alpha=0.7)\n",
        "axes[1].set_yticks(range(len(earnings_corr)))\n",
        "axes[1].set_yticklabels(earnings_corr.index)\n",
        "axes[1].set_xlabel('Correlation with 4-Year Earnings')\n",
        "axes[1].set_title('Feature Correlations with Earnings', fontsize=14, fontweight='bold')\n",
        "axes[1].axvline(0, color='black', linewidth=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_correlation.png', dpi=300)\n",
        "print(\"\\n✓ Saved: eda_correlation.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Finding: COSTT4_A vs TUITIONFEE_IN (r=0.98) → Will drop TUITIONFEE_IN\")"
      ],
      "metadata": {
        "id": "R-1nTNUjsGQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EDA: KEY VISUALIZATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create 6-panel visualization\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# 1. Earnings vs Cost scatter\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "has_both_data = merged_df_nops['EARN_MDN_4YR'].notna() & merged_df_nops['COSTT4_A'].notna()\n",
        "df_scatter = merged_df_nops[has_both_data].copy()\n",
        "\n",
        "control_colors = {1.0: 'green', 2.0: 'blue', 3.0: 'orange'}\n",
        "control_names = {1.0: 'Public', 2.0: 'Private', 3.0: 'For-Profit'}\n",
        "\n",
        "for control, color in control_colors.items():\n",
        "    mask = df_scatter['CONTROL'] == control\n",
        "    ax1.scatter(df_scatter[mask]['COSTT4_A'], df_scatter[mask]['EARN_MDN_4YR'],\n",
        "               alpha=0.3, s=5, c=color, label=control_names[control])\n",
        "ax1.plot([0, 90000], [0, 90000], 'r--', linewidth=2, alpha=0.5, label='Break-even')\n",
        "ax1.set_xlabel('Annual Cost ($)')\n",
        "ax1.set_ylabel('4-Year Earnings ($)')\n",
        "ax1.set_title('Earnings vs Cost by Institution Type')\n",
        "ax1.legend(fontsize=8)\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. Earnings distribution\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "earn_data = merged_df_nops['EARN_MDN_4YR'].dropna()\n",
        "ax2.hist(earn_data, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax2.axvline(earn_data.median(), color='red', linestyle='--', linewidth=2,\n",
        "           label=f'Median: ${earn_data.median():,.0f}')\n",
        "ax2.set_xlabel('4-Year Earnings ($)')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('4-Year Earnings Distribution')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Cost distribution\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "cost_data = merged_df_nops['COSTT4_A'].dropna()\n",
        "ax3.hist(cost_data, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
        "ax3.axvline(cost_data.median(), color='red', linestyle='--', linewidth=2,\n",
        "           label=f'Median: ${cost_data.median():,.0f}')\n",
        "ax3.set_xlabel('Annual Cost ($)')\n",
        "ax3.set_ylabel('Frequency')\n",
        "ax3.set_title('Annual Cost Distribution')\n",
        "ax3.legend()\n",
        "\n",
        "# 4. Programs by institution type\n",
        "ax4 = plt.subplot(2, 3, 4)\n",
        "control_counts = merged_df_nops['CONTROL'].value_counts().sort_index()\n",
        "labels = ['Public', 'Private', 'For-Profit']\n",
        "colors_bar = ['green', 'blue', 'orange']\n",
        "ax4.bar(labels, control_counts.values, color=colors_bar, alpha=0.7)\n",
        "ax4.set_ylabel('Number of Programs')\n",
        "ax4.set_title('Programs by Institution Type')\n",
        "for i, v in enumerate(control_counts.values):\n",
        "    ax4.text(i, v + 2000, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# 5. Programs by credential level\n",
        "ax5 = plt.subplot(2, 3, 5)\n",
        "cred_counts = merged_df_nops['CREDLEV'].value_counts().sort_index()\n",
        "cred_labels = {1: 'Cert', 2: 'Assoc', 3: 'Bach', 4: 'PB', 5: 'Master', 6: 'Doc', 7: '1st Prof', 8: 'Grad'}\n",
        "labels_cred = [cred_labels.get(int(x), str(x)) for x in cred_counts.index]\n",
        "ax5.bar(labels_cred, cred_counts.values, color='purple', alpha=0.7)\n",
        "ax5.set_ylabel('Number of Programs')\n",
        "ax5.set_title('Programs by Credential Level')\n",
        "ax5.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 6. Top majors\n",
        "ax6 = plt.subplot(2, 3, 6)\n",
        "top_majors = merged_df_nops['CIPDESC'].value_counts().head(10)\n",
        "ax6.barh(range(len(top_majors)), top_majors.values, color='teal', alpha=0.7)\n",
        "ax6.set_yticks(range(len(top_majors)))\n",
        "ax6.set_yticklabels([label[:35] + '...' if len(label) > 35 else label for label in top_majors.index], fontsize=8)\n",
        "ax6.set_xlabel('Number of Programs')\n",
        "ax6.set_title('Top 10 Most Common Majors')\n",
        "ax6.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_comprehensive.png', dpi=300, bbox_inches='tight')\n",
        "print(\" Saved: eda_comprehensive.png (6 plots)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N5UEGaH_TQYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPARE FOR TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Keep all programs with either earnings (no cost filter yet)\n",
        "df = merged_df_nops.copy()\n",
        "df_usable = df[(df['EARN_MDN_4YR'].notna()) | (df['EARN_MDN_1YR'].notna())].copy()\n",
        "\n",
        "print(f\"Original: {len(df):,}\")\n",
        "print(f\"With earnings: {len(df_usable):,}\")\n",
        "\n",
        "# Breakdown\n",
        "has_4yr = df_usable['EARN_MDN_4YR'].notna()\n",
        "has_1yr = df_usable['EARN_MDN_1YR'].notna()\n",
        "print(f\"\\n4-year only: {(has_4yr & ~has_1yr).sum():,}\")\n",
        "print(f\"1-year only: {(~has_4yr & has_1yr).sum():,}\")\n",
        "print(f\"Both: {(has_4yr & has_1yr).sum():,}\")\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "unique_units = df_usable['UNITID'].unique()\n",
        "train_units, test_units = train_test_split(unique_units, test_size=0.2, random_state=42)\n",
        "\n",
        "df_train = df_usable[df_usable['UNITID'].isin(train_units)].copy()\n",
        "df_test = df_usable[df_usable['UNITID'].isin(test_units)].copy()\n",
        "\n",
        "print(f\"\\nTrain: {len(df_train):,} | Test: {len(df_test):,}\")\n",
        "print(\"Split BEFORE imputation (no leakage)\")"
      ],
      "metadata": {
        "id": "03BrC5obTTkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CALCULATE IMPUTATION STATS (TRAIN ONLY)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Growth factors from train\n",
        "train_both = df_train[df_train['EARN_MDN_1YR'].notna() & df_train['EARN_MDN_4YR'].notna()].copy()\n",
        "train_both['GROWTH_FACTOR'] = train_both['EARN_MDN_4YR'] / train_both['EARN_MDN_1YR']\n",
        "train_both = train_both[(train_both['GROWTH_FACTOR'] >= 0.5) & (train_both['GROWTH_FACTOR'] <= 3.0)].copy()\n",
        "\n",
        "growth_factors = {\n",
        "    'overall': train_both['GROWTH_FACTOR'].median(),\n",
        "    'by_control': train_both.groupby('CONTROL')['GROWTH_FACTOR'].median().to_dict()\n",
        "}\n",
        "\n",
        "print(f\"Growth factors from {len(train_both):,} train programs:\")\n",
        "for c, g in growth_factors['by_control'].items():\n",
        "    print(f\"  CONTROL {c}: {g:.3f}x\")\n",
        "\n",
        "# Institutional medians from train\n",
        "inst_medians = {col: df_train.groupby('CONTROL')[col].median().to_dict()\n",
        "                for col in ['ADM_RATE', 'C150_4', 'PCTPELL', 'MD_EARN_WNE_P10']}\n",
        "\n",
        "print(\"Stats calculated from TRAIN only\")"
      ],
      "metadata": {
        "id": "qN-ohylkTaFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"APPLY IMPUTATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def impute_set(df, growth_factors, inst_medians):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Earnings imputation\n",
        "    df['EARN_4YR_SOURCE'] = 'missing'\n",
        "    df['EARN_MDN_4YR_FINAL'] = df['EARN_MDN_4YR'].copy()\n",
        "    df.loc[df['EARN_MDN_4YR'].notna(), 'EARN_4YR_SOURCE'] = 'original'\n",
        "\n",
        "    needs_impute = df['EARN_MDN_4YR'].isna() & df['EARN_MDN_1YR'].notna()\n",
        "    for idx in df[needs_impute].index:\n",
        "        control = df.loc[idx, 'CONTROL']\n",
        "        growth = growth_factors['by_control'].get(control, growth_factors['overall']) if pd.notna(control) else growth_factors['overall']\n",
        "        df.loc[idx, 'EARN_MDN_4YR_FINAL'] = df.loc[idx, 'EARN_MDN_1YR'] * growth\n",
        "        df.loc[idx, 'EARN_4YR_SOURCE'] = 'imputed'\n",
        "\n",
        "    # Institutional features\n",
        "    for col, medians in inst_medians.items():\n",
        "        for control, val in medians.items():\n",
        "            df.loc[(df['CONTROL']==control) & df[col].isna(), col] = val\n",
        "\n",
        "    df['SAMPLE_SIZE'] = df['EARN_COUNT_WNE_4YR'].fillna(df['EARN_COUNT_WNE_1YR'])\n",
        "    return df\n",
        "\n",
        "df_train = impute_set(df_train, growth_factors, inst_medians)\n",
        "df_test = impute_set(df_test, growth_factors, inst_medians)\n",
        "\n",
        "print(f\"Train - Original: {(df_train['EARN_4YR_SOURCE']=='original').sum():,}, Imputed: {(df_train['EARN_4YR_SOURCE']=='imputed').sum():,}\")\n",
        "print(f\"Test - Original: {(df_test['EARN_4YR_SOURCE']=='original').sum():,}, Imputed: {(df_test['EARN_4YR_SOURCE']=='imputed').sum():,}\")\n",
        "print(\"Imputation complete (no leakage)\")"
      ],
      "metadata": {
        "id": "QOWb_qkvTfyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"APPLY QUALITY FILTERS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def apply_filters(df):\n",
        "    return df[\n",
        "        (df['EARN_MDN_4YR_FINAL'].notna()) &\n",
        "        (df['EARN_MDN_4YR_FINAL'] > 10000) &\n",
        "        (df['EARN_MDN_4YR_FINAL'] < 350000) &\n",
        "        (df['COSTT4_A'].notna()) &\n",
        "        (df['COSTT4_A'] > 10000) &\n",
        "        (df['COSTT4_A'] < 400000) &\n",
        "        (df['SAMPLE_SIZE'] >= 5)\n",
        "    ].copy()\n",
        "\n",
        "df_train = apply_filters(df_train)\n",
        "df_test = apply_filters(df_test)\n",
        "\n",
        "print(f\"Train: {len(df_train):,} | Test: {len(df_test):,} | Total: {len(df_train)+len(df_test):,}\")\n",
        "print(\"Quality filters applied\")"
      ],
      "metadata": {
        "id": "QRvcHOJ9TnDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "    df['ROI_4YR'] = ((df['EARN_MDN_4YR_FINAL'] * 4 - df['COSTT4_A'] * 4) / (df['COSTT4_A'] * 4)) * 100\n",
        "    df['EARN_PREMIUM'] = df['EARN_MDN_4YR_FINAL'] / df['MD_EARN_WNE_P10']\n",
        "    return df\n",
        "\n",
        "df_train = create_features(df_train)\n",
        "df_test = create_features(df_test)\n",
        "\n",
        "print(f\"Created features: ROI_4YR, EARN_PREMIUM\")\n",
        "print(f\"\\nROI_4YR stats (train):\")\n",
        "print(df_train['ROI_4YR'].describe())\n",
        "\n",
        "# Visualize target\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_train['ROI_4YR'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "plt.axvline(df_train['ROI_4YR'].median(), color='red', linestyle='--', linewidth=2,\n",
        "           label=f\"Median: {df_train['ROI_4YR'].median():.1f}%\")\n",
        "plt.xlabel('ROI (%)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('ROI Distribution (Regression Target)')\n",
        "plt.legend()\n",
        "plt.savefig('target_distribution.png', dpi=300)\n",
        "print(\"\\nSaved: target_distribution.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SPgCgcGdThw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DROP REDUNDANT COLUMNS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "drop_cols = ['TUITIONFEE_IN', 'EARN_MDN_1YR', 'EARN_MDN_4YR', 'EARN_COUNT_WNE_1YR',\n",
        "             'EARN_COUNT_WNE_4YR', 'CIPDESC', 'CREDDESC']\n",
        "\n",
        "df_train = df_train.drop(columns=[c for c in drop_cols if c in df_train.columns])\n",
        "df_test = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])\n",
        "\n",
        "print(f\"Remaining columns: {df_train.shape[1]}\")\n",
        "print(\"Dropped redundant features\")"
      ],
      "metadata": {
        "id": "fGtA2m21T46s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENCODE CATEGORICALS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "target = 'ROI_4YR'\n",
        "\n",
        "# One-hot encode (fit on train, apply to both)\n",
        "small_cat = ['CREDLEV', 'CONTROL', 'REGION', 'DISTANCE', 'EARN_4YR_SOURCE']\n",
        "df_train_enc = pd.get_dummies(df_train, columns=small_cat, drop_first=True, dtype=int)\n",
        "df_test_enc = pd.get_dummies(df_test, columns=small_cat, drop_first=True, dtype=int)\n",
        "\n",
        "# Align columns\n",
        "df_train_enc, df_test_enc = df_train_enc.align(df_test_enc, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Target encode (using TRAIN means)\n",
        "cipcode_means = df_train.groupby('CIPCODE')[target].mean()\n",
        "df_train_enc['CIPCODE_ENCODED'] = df_train['CIPCODE'].map(cipcode_means)\n",
        "df_test_enc['CIPCODE_ENCODED'] = df_test['CIPCODE'].map(cipcode_means)\n",
        "\n",
        "stabbr_means = df_train.groupby('STABBR')[target].mean()\n",
        "df_train_enc['STABBR_ENCODED'] = df_train['STABBR'].map(stabbr_means)\n",
        "df_test_enc['STABBR_ENCODED'] = df_test['STABBR'].map(stabbr_means)\n",
        "\n",
        "df_train_enc = df_train_enc.drop(columns=['CIPCODE', 'STABBR'])\n",
        "df_test_enc = df_test_enc.drop(columns=['CIPCODE', 'STABBR'])\n",
        "\n",
        "print(f\"Encoded shape: {df_train_enc.shape}\")\n",
        "print(\"Encoding complete\")"
      ],
      "metadata": {
        "id": "Nij22p5kT-1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPARE FEATURES & TARGET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Separate X and y\n",
        "X_train = df_train_enc.drop(columns=[target, 'UNITID', 'INSTNM'])\n",
        "y_train = df_train_enc[target]\n",
        "\n",
        "X_test = df_test_enc.drop(columns=[target, 'UNITID', 'INSTNM'])\n",
        "y_test = df_test_enc[target]\n",
        "\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "N-AodLlmUCB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SCALE FEATURES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
        "\n",
        "print(f\" Scaled {len(numeric_cols)} features\")\n",
        "print(\"Ready for modeling\")"
      ],
      "metadata": {
        "id": "hCs_A5N0UGDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 1: TRAIN DEFAULT RANDOM FOREST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Default parameters\n",
        "rf_default = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Training with default parameters...\")\n",
        "print(\"  n_estimators: 100\")\n",
        "print(\"  max_depth: None (unlimited)\")\n",
        "print(\"  min_samples_split: 2\")\n",
        "print(\"  max_features: 'sqrt'\")\n",
        "\n",
        "rf_default.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = rf_default.predict(X_train_scaled)\n",
        "y_test_pred = rf_default.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEFAULT MODEL PERFORMANCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nTrain Set:\")\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "print(f\"  R²:   {train_r2:.4f}\")\n",
        "print(f\"  RMSE: {train_rmse:.2f}%\")\n",
        "print(f\"  MAE:  {train_mae:.2f}%\")\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "print(f\"  R²:   {test_r2:.4f}\")\n",
        "print(f\"  RMSE: {test_rmse:.2f}%\")\n",
        "print(f\"  MAE:  {test_mae:.2f}%\")\n",
        "\n",
        "# Check overfitting\n",
        "gap = train_r2 - test_r2\n",
        "print(f\"\\nOverfitting Check:\")\n",
        "print(f\"  Train R² - Test R²: {gap:.4f}\")\n",
        "if gap > 0.1:\n",
        "    print(\"  Overfitting detected (gap > 0.1)\")\n",
        "    print(\"  Hyperparameter tuning needed\")\n",
        "else:\n",
        "    print(\"  Reasonable generalization\")\n",
        "\n",
        "print(\"\\nDefault Random Forest trained\")"
      ],
      "metadata": {
        "id": "6uXGjBpwUOQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CHECK FOR LEAKED FEATURES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Print all features\n",
        "print(f\"\\nTotal features: {X_train.shape[1]}\")\n",
        "print(f\"\\nFeature list:\")\n",
        "for i, col in enumerate(X_train.columns, 1):\n",
        "    print(f\"  {i:3d}. {col}\")\n",
        "\n",
        "# Check for suspicious features\n",
        "suspicious = []\n",
        "for col in X_train.columns:\n",
        "    if 'EARN_MDN_4YR' in col:\n",
        "        suspicious.append(col)\n",
        "    if 'EARN_PREMIUM' in col:\n",
        "        suspicious.append(col)\n",
        "    if 'ROI' in col.upper():\n",
        "        suspicious.append(col)\n",
        "\n",
        "if suspicious:\n",
        "    print(f\"\\nLEAKED FEATURES FOUND:\")\n",
        "    for feat in suspicious:\n",
        "        print(f\" {feat}\")\n",
        "else:\n",
        "    print(\"\\n No obvious leaked features\")"
      ],
      "metadata": {
        "id": "890RUboYXL3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"CORRECTED FEATURE PREPARATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define leaked features (contain target information)\n",
        "leaked_features = [\n",
        "    'EARN_MDN_4YR_FINAL',      # Directly calculates ROI\n",
        "    'EARN_PREMIUM',             # Contains earnings\n",
        "    'SAMPLE_SIZE',              # Not known before enrollment\n",
        "    'EARN_4YR_SOURCE_original'  # Metadata only\n",
        "]\n",
        "\n",
        "print(\"Features to EXCLUDE (data leakage):\")\n",
        "for feat in leaked_features:\n",
        "    print(f\" {feat}\")\n",
        "\n",
        "# Separate X and y with correct features\n",
        "X_train = df_train_enc.drop(columns=['ROI_4YR', 'UNITID', 'INSTNM'] +\n",
        "                                     [f for f in leaked_features if f in df_train_enc.columns])\n",
        "y_train = df_train_enc['ROI_4YR']\n",
        "\n",
        "X_test = df_test_enc.drop(columns=['ROI_4YR', 'UNITID', 'INSTNM'] +\n",
        "                                   [f for f in leaked_features if f in df_test_enc.columns])\n",
        "y_test = df_test_enc['ROI_4YR']\n",
        "\n",
        "print(f\"\\nFeature matrix prepared\")\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  X_test: {X_test.shape}\")\n",
        "print(f\"  Features: {X_train.shape[1]} (removed {len(leaked_features)} leaked)\")\n",
        "\n",
        "print(\"\\nValid features (known before enrollment):\")\n",
        "for i, col in enumerate(X_train.columns[:10], 1):\n",
        "    print(f\"  {i}. {col}\")\n",
        "print(f\"  ... and {X_train.shape[1] - 10} more\")"
      ],
      "metadata": {
        "id": "bvWpqBi0OJpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SCALE FEATURES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
        "\n",
        "print(f\"Scaled {len(numeric_cols)} numeric features\")\n",
        "print(\" Scaler fit on TRAIN only\")\n",
        "print(\" Ready for modeling (NO DATA LEAKAGE)\")"
      ],
      "metadata": {
        "id": "aEpmw096OJ11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL: RANDOM FOREST REGRESSOR (DEFAULT)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Train with default parameters\n",
        "rf_default = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Training Random Forest with default parameters...\")\n",
        "print(\"  n_estimators: 100\")\n",
        "print(\"  max_depth: None (unlimited)\")\n",
        "print(\"  min_samples_split: 2\")\n",
        "print(\"  max_features: 'sqrt'\")\n",
        "\n",
        "rf_default.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = rf_default.predict(X_train_scaled)\n",
        "y_test_pred = rf_default.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEFAULT MODEL PERFORMANCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nTrain Set:\")\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "print(f\"  R²:   {train_r2:.4f}\")\n",
        "print(f\"  RMSE: {train_rmse:.2f}%\")\n",
        "print(f\"  MAE:  {train_mae:.2f}%\")\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "test_r2_default = r2_score(y_test, y_test_pred)\n",
        "test_rmse_default = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae_default = mean_absolute_error(y_test, y_test_pred)\n",
        "print(f\"  R²:   {test_r2_default:.4f}\")\n",
        "print(f\"  RMSE: {test_rmse_default:.2f}%\")\n",
        "print(f\"  MAE:  {test_mae_default:.2f}%\")\n",
        "\n",
        "# Overfitting check\n",
        "gap = train_r2 - test_r2_default\n",
        "print(f\"\\nOverfitting Check:\")\n",
        "print(f\"  Train R² - Test R²: {gap:.4f}\")\n",
        "if gap > 0.1:\n",
        "    print(\"  Overfitting detected - hyperparameter tuning recommended\")\n",
        "elif gap < 0:\n",
        "    print(\"  Underfitting - test performing better than train (unusual)\")\n",
        "else:\n",
        "    print(\"  Reasonable generalization\")\n",
        "\n",
        "print(\"\\nDefault Random Forest trained\")\n",
        "print(f\"Baseline established: R² = {test_r2_default:.4f}\")"
      ],
      "metadata": {
        "id": "mKDRyM7jOKEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "print(\"\"\"\n",
        "Strategy:\n",
        "─────────\n",
        "- Method: RandomizedSearchCV (faster than GridSearch)\n",
        "- Cross-validation: 5-fold on TRAIN set only\n",
        "- Scoring: R² (maximize explained variance)\n",
        "- Iterations: 20 random combinations\n",
        "\n",
        "Parameters to tune:\n",
        "───────────────────\n",
        "1. n_estimators [100, 200, 300]\n",
        "   More trees = better performance, diminishing returns after 200\n",
        "\n",
        "2. max_depth [10, 20, 30, None]\n",
        "   Controls tree depth, prevents overfitting\n",
        "\n",
        "3. min_samples_split [2, 5, 10]\n",
        "   Minimum samples to split node, regularization\n",
        "\n",
        "4. max_features ['sqrt', 'log2', 0.5]\n",
        "   Features per split, controls randomness\n",
        "\n",
        "5. min_samples_leaf [1, 2, 4]\n",
        "   Minimum samples in leaf, smooths predictions\n",
        "\"\"\")\n",
        "\n",
        "# Parameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'max_features': ['sqrt', 'log2', 0.5],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "print(f\"Total combinations: {3*4*3*3*3} = 324\")\n",
        "print(f\"Testing: 20 random combinations\")\n",
        "print(f\"Total model fits: 20 × 5 = 100\")\n",
        "\n",
        "# RandomizedSearchCV\n",
        "rf_random = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING (5-10 minutes)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "rf_random.fit(X_train_scaled, y_train)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\nTuning complete in {elapsed_time/60:.1f} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1W17Hb61OKTR",
        "outputId": "616f4d27-7d74-4ca1-fabe-74ad0b59d126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "HYPERPARAMETER TUNING\n",
            "======================================================================\n",
            "\n",
            "Strategy:\n",
            "─────────\n",
            "- Method: RandomizedSearchCV (faster than GridSearch)\n",
            "- Cross-validation: 5-fold on TRAIN set only\n",
            "- Scoring: R² (maximize explained variance)\n",
            "- Iterations: 20 random combinations\n",
            "\n",
            "Parameters to tune:\n",
            "───────────────────\n",
            "1. n_estimators [100, 200, 300]\n",
            "   More trees = better performance, diminishing returns after 200\n",
            "\n",
            "2. max_depth [10, 20, 30, None]\n",
            "   Controls tree depth, prevents overfitting\n",
            "\n",
            "3. min_samples_split [2, 5, 10]\n",
            "   Minimum samples to split node, regularization\n",
            "\n",
            "4. max_features ['sqrt', 'log2', 0.5]\n",
            "   Features per split, controls randomness\n",
            "\n",
            "5. min_samples_leaf [1, 2, 4]\n",
            "   Minimum samples in leaf, smooths predictions\n",
            "\n",
            "Total combinations: 324 = 324\n",
            "Testing: 20 random combinations\n",
            "Total model fits: 20 × 5 = 100\n",
            "\n",
            "======================================================================\n",
            "TRAINING (5-10 minutes)\n",
            "======================================================================\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1841381691.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}